{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/Qwen2-VL-jupyter/blob/main/Qwen2_VL_2B_Instruct.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "!apt -y install -qq aria2\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Qwen2-VL-7B-Instruct/raw/main/chat_template.json -d /content/Qwen2-VL-7B-Instruct/ -o chat_template.json \n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Qwen2-VL-7B-Instruct/raw/main/config.json -d /content/Qwen2-VL-7B-Instruct/ -o config.json \n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Qwen2-VL-7B-Instruct/raw/main/generation_config.json -d /content/Qwen2-VL-7B-Instruct/ -o generation_config.json \n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Qwen2-VL-7B-Instruct/raw/main/merges.txt -d /content/Qwen2-VL-7B-Instruct/ -o merges.txt \n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Qwen2-VL-7B-Instruct/resolve/main/model-00001-of-00005.safetensors -d /content/Qwen2-VL-7B-Instruct/ -o model-00001-of-00005.safetensors \n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Qwen2-VL-7B-Instruct/resolve/main/model-00002-of-00005.safetensors -d /content/Qwen2-VL-7B-Instruct/ -o model-00002-of-00005.safetensors \n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Qwen2-VL-7B-Instruct/resolve/main/model-00003-of-00005.safetensors -d /content/Qwen2-VL-7B-Instruct/ -o model-00003-of-00005.safetensors \n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Qwen2-VL-7B-Instruct/resolve/main/model-00004-of-00005.safetensors -d /content/Qwen2-VL-7B-Instruct/ -o model-00004-of-00005.safetensors \n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Qwen2-VL-7B-Instruct/resolve/main/model-00005-of-00005.safetensors -d /content/Qwen2-VL-7B-Instruct/ -o model-00005-of-00005.safetensors \n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Qwen2-VL-7B-Instruct/raw/main/model.safetensors.index.json -d /content/Qwen2-VL-7B-Instruct/ -o model.safetensors.index.json \n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Qwen2-VL-7B-Instruct/raw/main/preprocessor_config.json -d /content/Qwen2-VL-7B-Instruct/ -o preprocessor_config.json \n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Qwen2-VL-7B-Instruct/raw/main/tokenizer.json -d /content/Qwen2-VL-7B-Instruct/ -o tokenizer.json \n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Qwen2-VL-7B-Instruct/raw/main/tokenizer_config.json -d /content/Qwen2-VL-7B-Instruct/ -o tokenizer_config.json \n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Qwen2-VL-7B-Instruct/raw/main/vocab.json -d /content/Qwen2-VL-7B-Instruct/ -o vocab.json \n",
        "\n",
        "!pip install git+https://github.com/huggingface/transformers transformers-stream-generator==0.0.5 gradio==4.42.0 qwen-vl-utils==0.0.2 bitsandbytes==0.43.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy, re\n",
        "from threading import Thread\n",
        "import gradio as gr\n",
        "import torch\n",
        "\n",
        "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration, TextIteratorStreamer\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "checkpoint_path = \"/content/Qwen2-VL-7B-Instruct\"\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(checkpoint_path, device_map=\"auto\", load_in_8bit=True)\n",
        "processor = AutoProcessor.from_pretrained(checkpoint_path)\n",
        "\n",
        "def _parse_text(text):\n",
        "    lines = text.split('\\n')\n",
        "    lines = [line for line in lines if line != '']\n",
        "    count = 0\n",
        "    for i, line in enumerate(lines):\n",
        "        if '```' in line:\n",
        "            count += 1\n",
        "            items = line.split('`')\n",
        "            if count % 2 == 1:\n",
        "                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n",
        "            else:\n",
        "                lines[i] = '<br></code></pre>'\n",
        "        else:\n",
        "            if i > 0:\n",
        "                if count % 2 == 1:\n",
        "                    line = line.replace('`', r'\\`')\n",
        "                    line = line.replace('<', '&lt;')\n",
        "                    line = line.replace('>', '&gt;')\n",
        "                    line = line.replace(' ', '&nbsp;')\n",
        "                    line = line.replace('*', '&ast;')\n",
        "                    line = line.replace('_', '&lowbar;')\n",
        "                    line = line.replace('-', '&#45;')\n",
        "                    line = line.replace('.', '&#46;')\n",
        "                    line = line.replace('!', '&#33;')\n",
        "                    line = line.replace('(', '&#40;')\n",
        "                    line = line.replace(')', '&#41;')\n",
        "                    line = line.replace('$', '&#36;')\n",
        "                lines[i] = '<br>' + line\n",
        "    text = ''.join(lines)\n",
        "    return text\n",
        "\n",
        "def _remove_image_special(text):\n",
        "    text = text.replace('<ref>', '').replace('</ref>', '')\n",
        "    return re.sub(r'<box>.*?(</box>|$)', '', text)\n",
        "\n",
        "def _is_video_file(filename):\n",
        "    video_extensions = ['.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm', '.mpeg']\n",
        "    return any(filename.lower().endswith(ext) for ext in video_extensions)\n",
        "\n",
        "def _gc():\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def _transform_messages(original_messages):\n",
        "    transformed_messages = []\n",
        "    for message in original_messages:\n",
        "        new_content = []\n",
        "        for item in message['content']:\n",
        "            if 'image' in item:\n",
        "                new_item = {'type': 'image', 'image': item['image']}\n",
        "            elif 'text' in item:\n",
        "                new_item = {'type': 'text', 'text': item['text']}\n",
        "            elif 'video' in item:\n",
        "                new_item = {'type': 'video', 'video': item['video']}\n",
        "            else:\n",
        "                continue\n",
        "            new_content.append(new_item)\n",
        "\n",
        "        new_message = {'role': message['role'], 'content': new_content}\n",
        "        transformed_messages.append(new_message)\n",
        "\n",
        "    return transformed_messages\n",
        "\n",
        "def call_local_model(model, processor, messages):\n",
        "    messages = _transform_messages(messages)\n",
        "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "    inputs = processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors='pt')\n",
        "    inputs = inputs.to(model.device)\n",
        "    tokenizer = processor.tokenizer\n",
        "    streamer = TextIteratorStreamer(tokenizer, timeout=20.0, skip_prompt=True, skip_special_tokens=True)\n",
        "    gen_kwargs = {'max_new_tokens': 512, 'streamer': streamer, **inputs}\n",
        "    thread = Thread(target=model.generate, kwargs=gen_kwargs)\n",
        "    thread.start()\n",
        "    generated_text = ''\n",
        "    for new_text in streamer:\n",
        "        generated_text += new_text\n",
        "        yield generated_text\n",
        "\n",
        "def create_predict_fn():\n",
        "    def predict(_chatbot, task_history):\n",
        "        chat_query = _chatbot[-1][0]\n",
        "        query = task_history[-1][0]\n",
        "        if len(chat_query) == 0:\n",
        "            _chatbot.pop()\n",
        "            task_history.pop()\n",
        "            return _chatbot\n",
        "        print('User: ' + _parse_text(query))\n",
        "        history_cp = copy.deepcopy(task_history)\n",
        "        full_response = ''\n",
        "        messages = []\n",
        "        content = []\n",
        "        for q, a in history_cp:\n",
        "            if isinstance(q, (tuple, list)):\n",
        "                if _is_video_file(q[0]):\n",
        "                    content.append({'video': f'file://{q[0]}'})\n",
        "                else:\n",
        "                    content.append({'image': f'file://{q[0]}'})\n",
        "            else:\n",
        "                content.append({'text': q})\n",
        "                messages.append({'role': 'user', 'content': content})\n",
        "                messages.append({'role': 'assistant', 'content': [{'text': a}]})\n",
        "                content = []\n",
        "        messages.pop()\n",
        "\n",
        "        for response in call_local_model(model, processor, messages):\n",
        "            _chatbot[-1] = (_parse_text(chat_query), _remove_image_special(_parse_text(response)))\n",
        "\n",
        "            yield _chatbot\n",
        "            full_response = _parse_text(response)\n",
        "\n",
        "        task_history[-1] = (query, full_response)\n",
        "        print('Qwen-VL-Chat: ' + _parse_text(full_response))\n",
        "        yield _chatbot\n",
        "\n",
        "    return predict\n",
        "\n",
        "def create_regenerate_fn():\n",
        "\n",
        "    def regenerate(_chatbot, task_history):\n",
        "        if not task_history:\n",
        "            return _chatbot\n",
        "        item = task_history[-1]\n",
        "        if item[1] is None:\n",
        "            return _chatbot\n",
        "        task_history[-1] = (item[0], None)\n",
        "        chatbot_item = _chatbot.pop(-1)\n",
        "        if chatbot_item[0] is None:\n",
        "            _chatbot[-1] = (_chatbot[-1][0], None)\n",
        "        else:\n",
        "            _chatbot.append((chatbot_item[0], None))\n",
        "        _chatbot_gen = predict(_chatbot, task_history)\n",
        "        for _chatbot in _chatbot_gen:\n",
        "            yield _chatbot\n",
        "\n",
        "    return regenerate\n",
        "\n",
        "predict = create_predict_fn()\n",
        "regenerate = create_regenerate_fn()\n",
        "\n",
        "def add_text(history, task_history, text):\n",
        "    task_text = text\n",
        "    history = history if history is not None else []\n",
        "    task_history = task_history if task_history is not None else []\n",
        "    history = history + [(_parse_text(text), None)]\n",
        "    task_history = task_history + [(task_text, None)]\n",
        "    return history, task_history, ''\n",
        "\n",
        "def add_file(history, task_history, file):\n",
        "    history = history if history is not None else []\n",
        "    task_history = task_history if task_history is not None else []\n",
        "    history = history + [((file.name,), None)]\n",
        "    task_history = task_history + [((file.name,), None)]\n",
        "    return history, task_history\n",
        "\n",
        "def reset_user_input():\n",
        "    return gr.update(value='')\n",
        "\n",
        "def reset_state(_chatbot, task_history):\n",
        "    task_history.clear()\n",
        "    _chatbot.clear()\n",
        "    _gc()\n",
        "    return []\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"\"\"<p align=\"center\"><img src=\"https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png\" style=\"height: 80px\"/><p>\"\"\")\n",
        "    gr.Markdown(\"\"\"<center><font size=3>This WebUI is based on Qwen2_VL_7B_Instruct, developed by Alibaba Cloud.</center>\"\"\")\n",
        "    chatbot = gr.Chatbot(label='Qwen2-VL', elem_classes='control-height', height=900)\n",
        "    query = gr.Textbox(lines=2, label='Input')\n",
        "    task_history = gr.State([])\n",
        "\n",
        "    with gr.Row():\n",
        "        addfile_btn = gr.UploadButton('üìÅ Upload (‰∏ä‰º†Êñá‰ª∂)', file_types=['image', 'video'])\n",
        "        submit_btn = gr.Button('üöÄ Submit (ÂèëÈÄÅ)')\n",
        "        regen_btn = gr.Button('ü§îÔ∏è Regenerate (ÈáçËØï)')\n",
        "        empty_bin = gr.Button('üßπ Clear History (Ê∏ÖÈô§ÂéÜÂè≤)')\n",
        "\n",
        "    submit_btn.click(add_text, [chatbot, task_history, query],[chatbot, task_history]).then(predict, [chatbot, task_history], [chatbot], show_progress=True)\n",
        "    submit_btn.click(reset_user_input, [], [query])\n",
        "    empty_bin.click(reset_state, [chatbot, task_history], [chatbot], show_progress=True)\n",
        "    regen_btn.click(regenerate, [chatbot, task_history], [chatbot], show_progress=True)\n",
        "    addfile_btn.upload(add_file, [chatbot, task_history, addfile_btn], [chatbot, task_history], show_progress=True)\n",
        "    gr.Markdown(\"\"\"<font size=2>Note: This demo is governed by the original license of Qwen2-VL. \\\n",
        "                    We strongly advise users not to knowingly generate or allow others to knowingly generate harmful content, \\\n",
        "                    including hate speech, violence, pornography, deception, etc. \\\n",
        "                    (Ê≥®ÔºöÊú¨ÊºîÁ§∫ÂèóQwen2-VLÁöÑËÆ∏ÂèØÂçèËÆÆÈôêÂà∂„ÄÇÊàë‰ª¨Âº∫ÁÉàÂª∫ËÆÆÔºåÁî®Êà∑‰∏çÂ∫î‰º†Êí≠Âèä‰∏çÂ∫îÂÖÅËÆ∏‰ªñ‰∫∫‰º†Êí≠‰ª•‰∏ãÂÜÖÂÆπÔºå\\\n",
        "                    ÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫é‰ªáÊÅ®Ë®ÄËÆ∫„ÄÅÊö¥Âäõ„ÄÅËâ≤ÊÉÖ„ÄÅÊ¨∫ËØàÁõ∏ÂÖ≥ÁöÑÊúâÂÆ≥‰ø°ÊÅØ„ÄÇ)\"\"\")\n",
        "\n",
        "demo.queue().launch(share=True, debug=True, inline=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
